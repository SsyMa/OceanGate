\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}
% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{titling}
\usepackage{graphicx}
\usepackage{placeins}


\title{Deep Learning Homework \\
  [1ex] Deep Learning Házi feladat \\
  \large Airbus Ship Detection Challenge}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Nagy Dániel (UU5SCQ) \\
  Szénássy Márton (HIYXIQ) \\
  Varga-Labóczki Vazul (H947XW)
}


\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  This report presents our solution to the Airbus Ship Detection Challenge using a U-Net architecture for ship segmentation in satellite images.
  We discuss our initial attempts with Segment Anything and the challenges we faced, leading us to adopt the U-Net model.
  Our implementation includes various callbacks to enhance training, and we address challenges such as class imbalance through data augmentation.
  Overall, our solution demonstrates the effectiveness of deep learning techniques in addressing real-world problems in maritime surveillance.
\end{abstract}

\begin{kivonat}
  Ebben a dolgozatban bemutatjuk a megoldásunkat az Airbus Ship Detection Challenge kihívásra, amelyben U-Net architektúrát alkalmaztunk a műholdfelvételeken megjelenő hajók szegmentálására.
  Ismertetjük kezdeti kísérleteinket a Segment Anything modellel, valamint azokat a kihívásokat, amelyekkel szembesültünk, és amelyek arra késztettek minket, hogy a U-Net modellt válasszuk.
  Megvalósításunk különböző callback mechanizmusokat tartalmaz a tanulási folyamat javítása érdekében, és foglalkozunk olyan kihívásokkal, mint az osztályok közötti egyensúlyhiány, amelyet adatnövelési technikákkal kezelünk.
  Összességében megoldásunk bemutatja a mélytanulási technikák hatékonyságát a tengeri megfigyelés valós problémáinak kezelésében.
\end{kivonat}

\tableofcontents

\section{Introduction}
This report presents our solution to the Airbus Ship Detection Challenge~\cite{airbus2018challenge}.
This challenge focuses on the task of detecting ships in satellite images using deep learning techniques~\cite{goodfellow2016deep, lecun1998gradient}.
The challenge addresses a real-world problem in maritime surveillance and ocean monitoring, where automated detection systems can significantly reduce manual labor and improve response times.
We solved this problem by developing a convolutional neural network-based segmentation model that accurately identifies ship locations in satellite imagery.

\section{Description of topic and previous solutions}
The Airbus Ship Detection Challenge is a competition hosted on the Kaggle platform, aiming to develop algorithms for detecting ships in satellite images.
The challenge provides a dataset of high-resolution satellite images, each annotated with masks indicating the presence and location of ships.
Participants are tasked with creating models that can accurately segment ships from the background in these images. \\
Previous solutions include various deep learning architectures, such as U-Net~\cite{ronneberger2015unet}, Mask R-CNN~\cite{he2017maskrcnn} and Segment Anything~\cite{kirillov2023sam}.
These models leverage convolutional neural networks (CNNs) to learn spatial hierarchies of features from the input images.
Many participants have also employed data augmentation techniques to enhance model robustness and improve generalization to unseen data.


\section{Architecture}
\subsection{First attempt}
Our initial approach involved implementing Segment Anything~\cite{kirillov2023sam}, a state-of-the-art segmentation model known for its versatility and performance across various segmentation tasks.
However, we encountered significant challenges during this phase due to the model's way of working.
Segment Anything is designed to generate segmentation masks based on user-provided prompts, such as points or bounding boxes.
This interactive nature made it difficult to adapt the model for fully automated ship detection in satellite images, as required by the challenge.
This made it impractical for our specific use case, leading us to explore alternative architectures better suited for automated segmentation tasks.

\subsection{Final model}
\subsubsection{Model}
After evaluating various architectures, we decided to implement a U-Net model~\cite{ronneberger2015unet} for our ship detection task.
U-Net is a convolutional neural network architecture specifically designed for biomedical image segmentation but has proven effective in various segmentation tasks, including satellite imagery analysis.
The U-Net architecture consists of a contracting path (encoder) and an expansive path (decoder).
The encoder captures context and features from the input images through a series of convolutional and pooling layers~\cite{simonyan2014vgg}, while the decoder reconstructs the segmentation mask using upsampling and convolutional layers~\cite{springenberg2015striving}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Pictures/model.png}
    \caption{Model architecture.}
\end{figure}

\FloatBarrier

\subsubsection{Callbacks}
To enhance the training process and improve model performance, we incorporated several callbacks into our training routine:
\begin{itemize}
    \item \textbf{Model Checkpointing}: We used model checkpointing to save the best model weights based on validation loss during training. This ensures that we retain the most effective model configuration.
    \item \textbf{Early Stopping}: Early stopping was implemented to monitor the validation loss and halt training if no improvement was observed for a specified number of epochs. This helps prevent overfitting and reduces unnecessary computation.
    \item \textbf{Learning Rate Reduction}: We employed a learning rate reduction strategy that decreases the learning rate when the validation loss plateaus. This allows the model to fine-tune its weights more effectively during later stages of training.
\end{itemize}

\section{Implementation}
\subsection{Training}
We trained our U-Net model using the Adam optimizer~\cite{kingma2014adam} with a learning rate of 0.003 and a batch size of 32.
The training was conducted on our own hardware, utilizing a GPU to accelerate the training process.
We employed the binary cross-entropy loss function, which is suitable for binary segmentation tasks like ship detection.
The model was trained for 1000 epochs, with early stopping implemented to prevent overfitting.

\subsubsection{Dataset}
We utilized the Airbus Ship Detection Challenge dataset~\cite{airbus2018challenge}, which comprises high-resolution satellite images annotated with ship masks.
The dataset includes a diverse range of images, capturing various sea conditions, ship sizes, and orientations.

\subsubsection{Preprocessing}
To enhance the model's performance, we incorporated data augmentation techniques during training, such as random rotations, flips, and zooms.
This is the main task of the preprocessing step, as it increases the diversity of the training data and helps prevent overfitting.
Additionally, we applied batch normalization~\cite{ioffe2015batch} to stabilize the training process.

\subsubsection{Problems}
During the training process, we encountered several challenges that required careful consideration and adjustments to our approach.
One of the primary issues was dealing with class imbalance in the dataset, as the number of pixels representing ships was significantly lower than the background pixels.
In the begining we solved this problem by providing more images with ships during training.
In later trainings however, this problem was mitigated enough by data augmentation techniques, which helped to create a more balanced representation of ship pixels in the training data and the model was able to learn.

\section{Summary}
In this report, we presented our approach to the Airbus Ship Detection Challenge using a U-Net architecture for ship segmentation in satellite images.
We discussed our initial attempts with Segment Anything and the challenges we faced, leading us to adopt the U-Net model.
Our implementation included various callbacks to enhance training, and we addressed challenges such as class imbalance through data augmentation.
Overall, our solution demonstrates the effectiveness of deep learning techniques in addressing real-world problems in maritime surveillance.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}